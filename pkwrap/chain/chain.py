# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
#  Written by Srikanth Madikeri <srikanth.madikeri@idiap.ch>

# import sys
# import os
# import random
# from collections import OrderedDict, Counter
# import logging
# import argparse
# from dataclasses import dataclass
# from librosa.core.constantq import __num_two_factors
# import torch
# import torch.nn as nn
# from torch.nn.utils import clip_grad_value_
# import torch.optim as optim
# from _pkwrap import kaldi
# from . import chain
# from . import matrix
# from . import script_utils
# from collections import defaultdict, namedtuple
# import librosa
# import subprocess
# import io
# from math import ceil

# class KaldiChainObjfFunction(torch.autograd.Function):
#     """LF-MMI objective function for pytorch

#     This Function wraps MMI loss function implemented in Kaldi.
#     See Pytorch documentation of how to extend Function to check
#     the attributes present. I expect that this class will be used
#     as follows

#     ```
#         lfmmi_loss = KaldiChainObjfFunction.apply
#         ...
#         lfmmi_loss(chain_opts, den_graph, egs, nnet_output, xent_output)
#     ```
#     """
#     @staticmethod
#     def forward(ctx, opts, den_graph, supervision, nnet_output_tensor,
#                 xent_out_tensor):
#         """This function computes the loss for a single minibatch.

#         This function calls Kaldi's ComputeChainObjfAndDeriv through our
#         pybind11 wrapper. It takes the network outputs, rearranges them
#         in the way Kaldi expects, gets back the derivates of the outputs.
#         We pre-allocate the space for derivatives before passing to Kaldi.
#         No extra space is used by Kaldi as we pass only the poitners.

#         Args:
#             opts: training options for the loss function
#             den_graph: Denominator graph
#             supervision: merged egs for the current minibatch
#             nnet_output_tensor: output generated by the network
#             xent_out_tensor: the corresponding cross-entropy output

#         Returns:
#             We normally don't use the output returned by the function.
#             The derivatives are stored in the context and used by the backward()
#             function.
#         """
#         objf = torch.zeros(1, requires_grad=False)
#         l2_term = torch.zeros(1, requires_grad=False)
#         weight = torch.zeros(1, requires_grad=False)
#         mb, T, D = nnet_output_tensor.shape
#         # Kaldi expects the outputs to be groups by time frames. So
#         # we need to permut the output
#         nnet_output_copy = nnet_output_tensor.permute(1, 0, 2).reshape(-1, D).contiguous()
#         nnet_deriv = torch.zeros_like(nnet_output_copy).contiguous()
#         if xent_out_tensor is not None:
#             xent_deriv = torch.zeros_like(nnet_output_copy).contiguous()
#             kaldi.chain.ComputeChainObjfAndDeriv(
#                 opts,
#                 den_graph,
#                 supervision,
#                 nnet_output_copy,
#                 objf,
#                 l2_term,
#                 weight,
#                 nnet_deriv,
#                 xent_deriv,
#             )
#             nnet_deriv = nnet_deriv.reshape(T, mb, D).permute(1, 0, 2).contiguous()
#             xent_deriv = xent_deriv.reshape(T, mb, D).permute(1, 0, 2).contiguous()
#             xent_objf = (xent_out_tensor*xent_deriv).sum()/(mb*T)
#             objf[0] = objf[0]/weight[0]
#             logging.info(
#                 "objf={}, l2={}, xent_objf={}".format(
#                     objf[0],
#                     l2_term[0]/weight[0],
#                     xent_objf,
#                 )
#             )
#             ctx.save_for_backward(nnet_deriv, xent_deriv, torch.tensor(opts.xent_regularize, requires_grad=False))
#             logging.info("HERE xent_regularize is {}".format(opts.xent_regularize))
#         else:
#             kaldi.chain.ComputeChainObjfAndDerivNoXent(
#                 opts,
#                 den_graph,
#                 supervision,
#                 nnet_output_copy,
#                 objf,
#                 l2_term,
#                 weight,
#                 nnet_deriv,
#             )
#             nnet_deriv = nnet_deriv.reshape(T, mb, D).permute(1, 0, 2).contiguous()
#             xent_deriv = None
#             objf[0] = objf[0]/weight[0]
#             logging.info(
#                 "objf={}, l2={}".format(
#                     objf[0],
#                     l2_term[0]/weight[0],
#                 )
#             )
#             ctx.save_for_backward(nnet_deriv)
#         # return the derivates in the original order
#         return objf

#     @staticmethod
#     def backward(ctx, dummy):
#         """returns the derivatives"""
#         if len(ctx.saved_tensors) == 3:
#             nnet_deriv, xent_deriv, xent_regularize = ctx.saved_tensors
#             return None, None, None, -nnet_deriv, -xent_regularize*xent_deriv
#         else:
#             nnet_deriv = ctx.saved_tensors[0]
#             return None, None, None, -nnet_deriv, None


# class OnlineNaturalGradient(torch.autograd.Function):
#     """A wrapper to NG-SGD class in Kaldi

#     This class wraps Natural Gradient implemented in Kaldi by calling
#     nnet3's precondition_directions (wrapped through pybind11)
#     When implemented as an autograd Function we can easily wrap
#     it in a Linear layer. See pkwrap.nn.NaturalAffineTransform.
#     """
#     @staticmethod
#     def forward(ctx, input, weight, bias, in_state, out_state):
#         """Forward pass for NG-SGD layer

#         Args:
#             input: the input to the layer (a Tensor)
#             weight: weight matrix of the layer (a Tensor)
#             bias: the bias parameters of the layer (a Tensor)
#             in_state: state of the input (a kaldi.nnet3.OnlineNaturalGradient object)
#             out_state: state of the output (a kaldi.nnet3.OnlineNaturalGradient object)

#         Returns:
#             Linear transformation of the input with weight and bias.
#             The other inputs are saved in the context to be used during the call
#             to backward.
#         """
#         ctx.save_for_backward(input, weight, bias)
#         ctx.states = [in_state, out_state]
#         # the code below is based on pytorch's F.linear
#         if input.dim() == 2 and bias is not None:
#             output = torch.addmm(bias, input, weight.t())
#         else:
#             output = input.matmul(weight.t())
#             if bias is not None:
#                 output += bias
#         return output

#     @staticmethod
#     @torch.no_grad()
#     def backward(ctx, grad_output):
#         """Backward pass for NG-SGD layer

#         We pass the gradients computed by Pytorch to Kaldi's precondition_directions
#         given the states of the layer.
#         """
#         input, weight, _ = ctx.saved_tensors
#         in_state, out_state = ctx.states
#         if input.dim() == 3:
#             mb, T, D = input.shape
#             mb_T = mb*T
#         else:
#             mb_T, D = input.shape
#         input_temp = torch.zeros(mb_T, D+1, device=input.device, requires_grad=False).contiguous()
#         input_temp[:,-1] = 1.0
#         input_temp[:,:-1].copy_(input.reshape(mb_T, D))
#         grad_weight = grad_bias = None
#         if grad_output.dim() == 3:
#             grad_input = grad_output.matmul(weight)
#             grad_input = grad_input.reshape(mb, T, D)
#         else:
#             grad_input = grad_output.mm(weight)
#         in_scale = kaldi.nnet3.precondition_directions(in_state, input_temp)
#         out_dim = grad_output.shape[-1]
#         grad_output_temp = grad_output.view(-1, out_dim)
#         out_scale = kaldi.nnet3.precondition_directions(out_state, grad_output_temp) # hope grad_output is continguous!
#         scale = in_scale*out_scale
#         grad_output.data.mul_(scale)
#         # TODO: check if we should use data member instead?
#         grad_weight = grad_output_temp.t().mm(input_temp[:,:-1])
#         grad_bias = grad_output_temp.t().mm(input_temp[:,-1].reshape(-1,1))
#         grad_weight.data.mul_(scale)
#         grad_bias.data.mul_(scale)
#         return grad_input, grad_weight, grad_bias.t(), None, None


